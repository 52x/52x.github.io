<!DOCTYPE html>
<html lang="zh">
    <head>
    <!--
        © Material Theme
        https://github.com/viosey/hexo-theme-material
        Version: 1.3.4 -->

    <!-- Title -->
    
    <title>
        
            (机器学习) 神经网络与深度学习笔记-深度学习 | 
        
        Hexo
    </title>

    <!-- Meta & Info -->
    <meta charset="utf-8">

    <!-- dns prefetch -->
    <meta http-equiv="x-dns-prefetch-control" content="on">
    
    
    
    
    
    

    <meta http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#0097A7">
    <meta name="author" content="John Doe">
    <meta name="description" content="null">
    <meta name="keywords" content="null,Math,Machine Learning,Deep Learning">

    <!-- Favicons -->
    <link rel="icon shortcut" type="image/ico" href="/img/favicon.png">
    <link rel="icon" sizes="192x192" href="/img/favicon.png">
    <link rel="apple-touch-icon" href="/img/favicon.png">

    <!--iOS -->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-title" content="Title">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="480">

    <!-- Add to homescreen for Chrome on Android -->
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Add to homescreen for Safari on iOS -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="Hexo">

    <!-- The Open Graph protocol -->
    <meta property="og:url" content="http://yoursite.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="(机器学习) 神经网络与深度学习笔记-深度学习 | Hexo">
    <meta property="og:description" content="null">
    <meta property="og:article:tag" content="Math"> <meta property="og:article:tag" content="Machine Learning"> <meta property="og:article:tag" content="Deep Learning"> 

    <!--[if lte IE 9]>
        <link rel="stylesheet" href="/css/ie-blocker.css">

        
            <script src="/js/ie-blocker.zhCN.js"></script>
        
    <![endif]-->

    <!-- Import CSS & jQuery -->
    
        <link rel="stylesheet" href="/css/material.min.css">
        <link rel="stylesheet" href="/css/style.min.css">
        <!-- Config CSS -->


<!-- Other Styles -->
<style>
  body, html {
    font-family: Roboto, "Helvetica Neue", Helvetica, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "微软雅黑", Arial, sans-serif;
  }

  a {
    color: #00838F;
  }

  .mdl-card__media,
  #search-label,
  #search-form-label:after,
  #scheme-Paradox .hot_tags-count,
  #scheme-Paradox .sidebar_archives-count,
  #scheme-Paradox .sidebar-colored .sidebar-header,
  #scheme-Paradox .sidebar-colored .sidebar-badge{
    background-color: #0097A7 !important;
  }

  /* Sidebar User Drop Down Menu Text Color */
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:hover,
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:focus {
    color: #0097A7 !important;
  }

  #post_entry-right-info,
  .sidebar-colored .sidebar-nav li:hover > a,
  .sidebar-colored .sidebar-nav li:hover > a i,
  .sidebar-colored .sidebar-nav li > a:hover,
  .sidebar-colored .sidebar-nav li > a:hover i,
  .sidebar-colored .sidebar-nav li > a:focus i,
  .sidebar-colored .sidebar-nav > .open > a,
  .sidebar-colored .sidebar-nav > .open > a:hover,
  .sidebar-colored .sidebar-nav > .open > a:focus,
  #ds-reset #ds-ctx .ds-ctx-entry .ds-ctx-head a {
    color: #0097A7 !important;
  }

  .toTop {
    background: #757575 !important;
  }

  .material-layout .material-post>.material-nav,
  .material-layout .material-index>.material-nav,
  .material-nav a {
    color: #757575;
  }

  #scheme-Paradox .MD-burger-layer {
    background-color: #757575;
  }

  #scheme-Paradox #post-toc-trigger-btn {
    color: #757575;
  }

  .post-toc a:hover {
    color: #00838F;
    text-decoration: underline;
  }

</style>


<!-- Theme Background Related-->

    <style>
      body{
        background-color: #F5F5F5;
      }

      /* blog_info bottom background */
      #scheme-Paradox .material-layout .something-else .mdl-card__supporting-text{
        background-color: #fff;
      }
    </style>




<!-- Fade Effect -->

    <style>
      .fade {
        transition: all 800ms linear;
        -webkit-transform: translate3d(0,0,0);
        -moz-transform: translate3d(0,0,0);
        -ms-transform: translate3d(0,0,0);
        -o-transform: translate3d(0,0,0);
        transform: translate3d(0,0,0);
        opacity: 1;
      }

      .fade.out{
        opacity: 0;
      }
    </style>


        <script src="/js/jquery.min.js"></script>
        <script src="/js/queue.js"></script>
    

    <!-- UC Browser Compatible -->
    <script>
        var agent = navigator.userAgent.toLowerCase();
        if(agent.indexOf('ucbrowser')>0) {
            document.write("<link rel=\"stylesheet\" href=\"/css/uc.css\">");
            alert('由于 UC 浏览器使用极旧的内核，而本网站使用了一些新的特性。\n为了您能更好的浏览，推荐使用 Chrome 或 Firefox 浏览器。');
        }
    </script>

    

    


    <!-- Bing Background -->
    

    <!-- Custom Head -->
    
</head>


    
        <body id="scheme-Paradox" class="lazy">
            <div class="material-layout  mdl-js-layout has-drawer is-upgraded">
                

                <!-- Main Container -->
                <main class="material-layout__content" id="main">

                    <!-- Top Anchor -->
                    <div id="top"></div>

                    
                        <!-- Hamburger Button -->
                        <button class="MD-burger-icon sidebar-toggle">
                            <span class="MD-burger-layer"></span>
                        </button>
                    

                    <!-- Post TOC -->

    
    <!-- Back Button -->
    <!--
    <div class="material-back" id="backhome-div" tabindex="0">
        <a class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"
           href="#" onclick="window.history.back();return false;"
           target="_self"
           role="button"
           data-upgraded=",MaterialButton,MaterialRipple">
            <i class="material-icons" role="presentation">arrow_back</i>
            <span class="mdl-button__ripple-container">
                <span class="mdl-ripple"></span>
            </span>
        </a>
    </div>
    -->

    <!-- Left aligned menu below button -->
    <button id="post-toc-trigger-btn"
        class="mdl-button mdl-js-button mdl-button--icon">
        <i class="material-icons">format_list_numbered</i>
    </button>

    <ul class="post-toc-wrap mdl-menu mdl-menu--bottom-left mdl-js-menu mdl-js-ripple-effect" for="post-toc-trigger-btn" style="max-height:80vh; overflow-y:scroll;">
        <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.</span> <span class="post-toc-text">Chapters 4 ~ 6 Deep Learning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Neural nets can compute any function</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">Why are deep neural networks hard to train?</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.2.1.</span> <span class="post-toc-text">The prevalence of the vanishing gradient problem.</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">Deep Learning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.3.1.</span> <span class="post-toc-text">Convolutional networks</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">Code for convolutional network</span></a></li></ol></li></ol>

        <!--
        <li class="mdl-menu__item">
            Some Action
        </li>
        -->
    </ul>




<!-- Layouts -->

    <!-- Post Module -->
    <div class="material-post_container">

        <div class="material-post mdl-grid">
            <div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col">

                <!-- Post Header(Thumbnail & Title) -->
                
    <!-- Paradox Post Header -->
    
        
            <!-- Random Thumbnail -->
            <div class="post_thumbnail-random mdl-card__media mdl-color-text--grey-50">
            <script>
    var randomNum = Math.floor(Math.random() * 19 + 1);

    $('.post_thumbnail-random').attr('data-original', '/img/random/material-' + randomNum + '.png');
    $('.post_thumbnail-random').addClass('lazy');
</script>

        
    
            <p class="article-headline-p">
                (机器学习) 神经网络与深度学习笔记-深度学习
            </p>
        </div>





                
                    <!-- Paradox Post Info -->
                    <div class="mdl-color-text--grey-700 mdl-card__supporting-text meta">

    <!-- Author Avatar -->
    <div id="author-avatar">
        <img src="/img/avatar.png" width="44px" height="44px" alt="Author Avatar"/>
    </div>
    <!-- Author Name & Date -->
    <div>
        <strong>John Doe</strong>
        <span>3月 12, 2017</span>
    </div>

    <div class="section-spacer"></div>

    <!-- Favorite -->
    <!--
        <button id="article-functions-like-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon btn-like">
            <i class="material-icons" role="presentation">favorite</i>
            <span class="visuallyhidden">favorites</span>
        </button>
    -->

    <!-- Qrcode -->
    

    <!-- Tags (bookmark) -->
    
    <button id="article-functions-viewtags-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
        <i class="material-icons" role="presentation">bookmark</i>
        <span class="visuallyhidden">bookmark</span>
    </button>
    <ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-viewtags-button">
        <li class="mdl-menu__item">
        <a class="post_tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="mdl-menu__item"><a class="post_tag-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="mdl-menu__item"><a class="post_tag-link" href="/tags/Math/">Math</a>
    </ul>
    

    <!-- Share -->
    <button id="article-fuctions-share-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">share</i>
    <span class="visuallyhidden">share</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-fuctions-share-button">
    

    

    <!-- Share Weibo -->
    
        <a class="post_share-link" href="http://service.weibo.com/share/share.php?appkey=&title=(机器学习) 神经网络与深度学习笔记-深度学习&url=http://yoursite.com//2017/03/12/note_nndl_deep_learning/index.html&pic=&searchPic=false&style=simple" target="_blank">
            <li class="mdl-menu__item">
                分享到微博
            </li>
        </a>
    

    <!-- Share Twitter -->
    
        <a class="post_share-link" href="https://twitter.com/intent/tweet?text=(机器学习) 神经网络与深度学习笔记-深度学习&url=http://yoursite.com//2017/03/12/note_nndl_deep_learning/index.html&via=John Doe" target="_blank">
            <li class="mdl-menu__item">
                分享到 Twitter
            </li>
        </a>
    

    <!-- Share Facebook -->
    
        <a class="post_share-link" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com//2017/03/12/note_nndl_deep_learning/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Facebook
            </li>
        </a>
    

    <!-- Share Google+ -->
    
        <a class="post_share-link" href="https://plus.google.com/share?url=http://yoursite.com//2017/03/12/note_nndl_deep_learning/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Google+
            </li>
        </a>
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    

    <!-- Share Telegram -->
    
</ul>

</div>

                

                <!-- Post Content -->
                <div id="post-content" class="mdl-color-text--grey-700 mdl-card__supporting-text fade out">
    
        <h1>Chapters 4 ~ 6 Deep Learning</h1>
<h2>Neural nets can compute any function</h2>
<p>2 caveats about universality</p>
<ul>
<li>given any function $f(x)$ and accuracy $\epsilon$ , using enough hidden neurons we can always find a neural network whose output $g(x)$ staisfies $\vert g(x) - f(x) \vert &lt; \epsilon$ for all inputs $x$. Moreover, we can achieve this with a single hidden layer.</li>
<li>the class of functions which can be approximated are <em>continuous</em> functions.</li>
</ul>
<p>Summing up, neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.</p>
<blockquote>
<p>Despite of the fact above, empirical evidence suggests that deep networks are the networks best adapted to learn the functions useful in solving many real-world problems.</p>
</blockquote>
<p>&lt;!--more--&gt;</p>
<h2>Why are deep neural networks hard to train?</h2>
<p>One of the obstacle for training deep neural networks is <code>vanishing gradient</code>.</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz38.png" alt=""></p>
<p>Consider the simplest deep neural network shown above. Where $w_i$ and $b_i$ stand for weights and biases, $\sigma$ is the sigmoid activation function, and $C$ is the loss to be minimized.</p>
<p>The derivative reaches a maximum at $\sigma'(0) = 0.25$, and we usually initialize the weights so  $\vert w_i \vert &lt; 1$, thus the gradient get smaller when the error is propagated to the earlier layers. This is the essential origin of the vanishing gradient problem.</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz39.png" alt=""></p>
<p>If the weights grow during training, we might have an exploding gradient problem instead.</p>
<blockquote>
<p>The real problem here is that neural networks suffer from an <em>unstable gradient problem</em>. As a result, if we use standard gradient-based learning techniques, different layers in the network will tend to learn at wildly different speeds.</p>
</blockquote>
<p>In a more general deep network, the same behavior occurs as well</p>
<p>$$\delta^l = \Sigma'(z^l) (w^{l+1})^T \Sigma'(z^{l+1}) (w^{l+2})^T \ldots  \Sigma'(z^L) \nabla_a C$$</p>
<p>where the $\Sigma'(z^l)$ is a diagonal matrix whose entries are the $\sigma'(z)$ values for the weighted inputs to the <em>l</em>th layer. In practice, empirically it is typically found in sigmoid networks that gradients vanish exponentially quickly in earlier layers. As a result, learning slows down in those layers.</p>
<h3>The prevalence of the vanishing gradient problem.</h3>
<p>Gradient can either vanish or explode in the early layers of a deep network, but usually they vanish. Because if we make the weights large, we might inevitably increased the $z = wa+b$ at the same time, which makes $\sigma'(z)$ very small. The only way to avoid this is to limit the input activation within a fairly narrow range of values, and most of the time it won't happen.</p>
<h2>Deep Learning</h2>
<h3>Convolutional networks</h3>
<p>CNN use three basic ideas</p>
<ul>
<li>local receptive fields, each neuron in the hidden layer will be connected to some sliding region of the previous layer. The sliding action might use different <em>stride</em>, or use <em>zero-padding</em>.</li>
<li>shared weights and biases, all neurons in the same hidden layer detect exactly the same feature.</li>
<li>pooling, pooling layers are usually used immediately after convolutional layers to simplify the information in the output from convolutional layer.</li>
</ul>
<p><img src="http://neuralnetworksanddeeplearning.com/images/simple_conv.png" alt=""></p>
<p>Some results of experiment in this chapter, what helped to increase the accuracy</p>
<ul>
<li>Using convolutional layers with pooling layers</li>
<li>Replacing sigmoid with ReLU activations</li>
<li>Data augmentation</li>
<li>Insert fully connected layers with dropout after convolutional layers</li>
<li>Using ensemble of networks even the only difference is random initialization</li>
</ul>
<blockquote>
<p>Regarding the use of ReLU, the reason is still empirical. We still don't have a theory to pick the best activation function. ReLU speed up the training over sigmoid by a factor of 3-5.</p>
<p>Convolutional layers don't need dropout because they are less likely to pick up on local features.</p>
</blockquote>
<p>Actually, the vanishing gradient problem hasn't be theoretically solved. But combining ReLU and GPU computing, we speed up the training tens of times faster than before, which makes it possible for early layers to learn. Together with regularization techniques such as convolutional layers and dropout, we reduced the overfitting and increased the accuracy substantially.</p>
<h2>Code for convolutional network</h2>
<p>The code is implemented with <code>Theano</code>.</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""network3.py</span></div><div class="line">~~~~~~~~~~~~~~</div><div class="line"></div><div class="line">A Theano-based program for training and running simple neural</div><div class="line">networks.</div><div class="line"></div><div class="line">Supports several layer types (fully connected, convolutional, max</div><div class="line">pooling, softmax), and activation functions (sigmoid, tanh, and</div><div class="line">rectified linear units, with more easily added).</div><div class="line"></div><div class="line">When run on a CPU, this program is much faster than network.py and</div><div class="line">network2.py.  However, unlike network.py and network2.py it can also</div><div class="line">be run on a GPU, which makes it faster still.</div><div class="line"></div><div class="line">Because the code is based on Theano, the code is different in many</div><div class="line">ways from network.py and network2.py.  However, where possible I have</div><div class="line">tried to maintain consistency with the earlier programs.  In</div><div class="line">particular, the API is similar to network2.py.  Note that I have</div><div class="line">focused on making the code simple, easily readable, and easily</div><div class="line">modifiable.  It is not optimized, and omits many desirable features.</div><div class="line"></div><div class="line">This program incorporates ideas from the Theano documentation on</div><div class="line">convolutional neural nets (notably,</div><div class="line">http://deeplearning.net/tutorial/lenet.html ), from Misha Denil's</div><div class="line">implementation of dropout (https://github.com/mdenil/dropout ), and</div><div class="line">from Chris Olah (http://colah.github.io ).</div><div class="line"></div><div class="line">Written for Theano 0.6 and 0.7, needs some changes for more recent</div><div class="line">versions of Theano.</div><div class="line"></div><div class="line">"""</div><div class="line"></div><div class="line"><span class="comment">#### Libraries</span></div><div class="line"><span class="comment"># Standard library</span></div><div class="line"><span class="keyword">import</span> cPickle</div><div class="line"><span class="keyword">import</span> gzip</div><div class="line"></div><div class="line"><span class="comment"># Third-party libraries</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> theano</div><div class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</div><div class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> conv</div><div class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> softmax</div><div class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> shared_randomstreams</div><div class="line"><span class="keyword">from</span> theano.tensor.signal <span class="keyword">import</span> downsample</div><div class="line"></div><div class="line"><span class="comment"># Activation functions for neurons</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear</span><span class="params">(z)</span>:</span> <span class="keyword">return</span> z</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReLU</span><span class="params">(z)</span>:</span> <span class="keyword">return</span> T.maximum(<span class="number">0.0</span>, z)</div><div class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> sigmoid</div><div class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> tanh</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#### Constants</span></div><div class="line">GPU = <span class="keyword">True</span></div><div class="line"><span class="keyword">if</span> GPU:</div><div class="line">    <span class="keyword">print</span> <span class="string">"Trying to run under a GPU.  If this is not desired, then modify "</span>+\</div><div class="line">        <span class="string">"network3.py\nto set the GPU flag to False."</span></div><div class="line">    <span class="keyword">try</span>: theano.config.device = <span class="string">'gpu'</span></div><div class="line">    <span class="keyword">except</span>: <span class="keyword">pass</span> <span class="comment"># it's already set</span></div><div class="line">    theano.config.floatX = <span class="string">'float32'</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">print</span> <span class="string">"Running with a CPU.  If this is not desired, then the modify "</span>+\</div><div class="line">        <span class="string">"network3.py to set\nthe GPU flag to True."</span></div><div class="line"></div><div class="line"><span class="comment">#### Load the MNIST data</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_shared</span><span class="params">(filename=<span class="string">"../data/mnist.pkl.gz"</span>)</span>:</span></div><div class="line">    f = gzip.open(filename, <span class="string">'rb'</span>)</div><div class="line">    training_data, validation_data, test_data = cPickle.load(f)</div><div class="line">    f.close()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shared</span><span class="params">(data)</span>:</span></div><div class="line">        <span class="string">"""Place the data into shared variables.  This allows Theano to copy</span></div><div class="line">        the data to the GPU, if one is available.</div><div class="line"></div><div class="line">        """</div><div class="line">        shared_x = theano.shared(</div><div class="line">            np.asarray(data[<span class="number">0</span>], dtype=theano.config.floatX), borrow=<span class="keyword">True</span>)</div><div class="line">        shared_y = theano.shared(</div><div class="line">            np.asarray(data[<span class="number">1</span>], dtype=theano.config.floatX), borrow=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">"int32"</span>)</div><div class="line">    <span class="keyword">return</span> [shared(training_data), shared(validation_data), shared(test_data)]</div><div class="line"></div><div class="line"><span class="comment">#### Main class used to construct and train networks</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, mini_batch_size)</span>:</span></div><div class="line">        <span class="string">"""Takes a list of `layers`, describing the network architecture, and</span></div><div class="line">        a value for the `mini_batch_size` to be used during training</div><div class="line">        by stochastic gradient descent.</div><div class="line"></div><div class="line">        """</div><div class="line">        self.layers = layers</div><div class="line">        self.mini_batch_size = mini_batch_size</div><div class="line">        self.params = [param <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers <span class="keyword">for</span> param <span class="keyword">in</span> layer.params]</div><div class="line">        self.x = T.matrix(<span class="string">"x"</span>)</div><div class="line">        self.y = T.ivector(<span class="string">"y"</span>)</div><div class="line">        init_layer = self.layers[<span class="number">0</span>]</div><div class="line">        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, len(self.layers)):</div><div class="line">            prev_layer, layer  = self.layers[j<span class="number">-1</span>], self.layers[j]</div><div class="line">            layer.set_inpt(</div><div class="line">                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)</div><div class="line">        self.output = self.layers[<span class="number">-1</span>].output</div><div class="line">        self.output_dropout = self.layers[<span class="number">-1</span>].output_dropout</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></div><div class="line">            validation_data, test_data, lmbda=<span class="number">0.0</span>):</div><div class="line">        <span class="string">"""Train the network using mini-batch stochastic gradient descent."""</span></div><div class="line">        training_x, training_y = training_data</div><div class="line">        validation_x, validation_y = validation_data</div><div class="line">        test_x, test_y = test_data</div><div class="line"></div><div class="line">        <span class="comment"># compute number of minibatches for training, validation and testing</span></div><div class="line">        num_training_batches = size(training_data)/mini_batch_size</div><div class="line">        num_validation_batches = size(validation_data)/mini_batch_size</div><div class="line">        num_test_batches = size(test_data)/mini_batch_size</div><div class="line"></div><div class="line">        <span class="comment"># define the (regularized) cost function, symbolic gradients, and updates</span></div><div class="line">        l2_norm_squared = sum([(layer.w**<span class="number">2</span>).sum() <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers])</div><div class="line">        cost = self.layers[<span class="number">-1</span>].cost(self)+\</div><div class="line">               <span class="number">0.5</span>*lmbda*l2_norm_squared/num_training_batches</div><div class="line">        grads = T.grad(cost, self.params)</div><div class="line">        updates = [(param, param-eta*grad)</div><div class="line">                   <span class="keyword">for</span> param, grad <span class="keyword">in</span> zip(self.params, grads)]</div><div class="line"></div><div class="line">        <span class="comment"># define functions to train a mini-batch, and to compute the</span></div><div class="line">        <span class="comment"># accuracy in validation and test mini-batches.</span></div><div class="line">        i = T.lscalar() <span class="comment"># mini-batch index</span></div><div class="line">        train_mb = theano.function(</div><div class="line">            [i], cost, updates=updates,</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                training_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">                self.y:</div><div class="line">                training_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line">        validate_mb_accuracy = theano.function(</div><div class="line">            [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                validation_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">                self.y:</div><div class="line">                validation_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line">        test_mb_accuracy = theano.function(</div><div class="line">            [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">                self.y:</div><div class="line">                test_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line">        self.test_mb_predictions = theano.function(</div><div class="line">            [i], self.layers[<span class="number">-1</span>].y_out,</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line">        <span class="comment"># Do the actual training</span></div><div class="line">        best_validation_accuracy = <span class="number">0.0</span></div><div class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</div><div class="line">            <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> xrange(num_training_batches):</div><div class="line">                iteration = num_training_batches*epoch+minibatch_index</div><div class="line">                <span class="keyword">if</span> iteration % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">                    print(<span class="string">"Training mini-batch number &#123;0&#125;"</span>.format(iteration))</div><div class="line">                cost_ij = train_mb(minibatch_index)</div><div class="line">                <span class="keyword">if</span> (iteration+<span class="number">1</span>) % num_training_batches == <span class="number">0</span>:</div><div class="line">                    validation_accuracy = np.mean(</div><div class="line">                        [validate_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)])</div><div class="line">                    print(<span class="string">"Epoch &#123;0&#125;: validation accuracy &#123;1:.2%&#125;"</span>.format(</div><div class="line">                        epoch, validation_accuracy))</div><div class="line">                    <span class="keyword">if</span> validation_accuracy &gt;= best_validation_accuracy:</div><div class="line">                        print(<span class="string">"This is the best validation accuracy to date."</span>)</div><div class="line">                        best_validation_accuracy = validation_accuracy</div><div class="line">                        best_iteration = iteration</div><div class="line">                        <span class="keyword">if</span> test_data:</div><div class="line">                            test_accuracy = np.mean(</div><div class="line">                                [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)])</div><div class="line">                            print(<span class="string">'The corresponding test accuracy is &#123;0:.2%&#125;'</span>.format(</div><div class="line">                                test_accuracy))</div><div class="line">        print(<span class="string">"Finished training network."</span>)</div><div class="line">        print(<span class="string">"Best validation accuracy of &#123;0:.2%&#125; obtained at iteration &#123;1&#125;"</span>.format(</div><div class="line">            best_validation_accuracy, best_iteration))</div><div class="line">        print(<span class="string">"Corresponding test accuracy of &#123;0:.2%&#125;"</span>.format(test_accuracy))</div><div class="line"></div><div class="line"><span class="comment">#### Define layer types</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvPoolLayer</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Used to create a combination of a convolutional and a max-pooling</span></div><div class="line">    layer.  A more sophisticated implementation would separate the</div><div class="line">    two, but for our purposes we'll always use them together, and it</div><div class="line">    simplifies the code, so it makes sense to combine them.</div><div class="line"></div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filter_shape, image_shape, poolsize=<span class="params">(<span class="number">2</span>, <span class="number">2</span>)</span>,</span></span></div><div class="line">                 activation_fn=sigmoid):</div><div class="line">        <span class="string">"""`filter_shape` is a tuple of length 4, whose entries are the number</span></div><div class="line">        of filters, the number of input feature maps, the filter height, and the</div><div class="line">        filter width.</div><div class="line"></div><div class="line">        `image_shape` is a tuple of length 4, whose entries are the</div><div class="line">        mini-batch size, the number of input feature maps, the image</div><div class="line">        height, and the image width.</div><div class="line"></div><div class="line">        `poolsize` is a tuple of length 2, whose entries are the y and</div><div class="line">        x pooling sizes.</div><div class="line"></div><div class="line">        """</div><div class="line">        self.filter_shape = filter_shape</div><div class="line">        self.image_shape = image_shape</div><div class="line">        self.poolsize = poolsize</div><div class="line">        self.activation_fn=activation_fn</div><div class="line">        <span class="comment"># initialize weights and biases</span></div><div class="line">        n_out = (filter_shape[<span class="number">0</span>]*np.prod(filter_shape[<span class="number">2</span>:])/np.prod(poolsize))</div><div class="line">        self.w = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(loc=<span class="number">0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=filter_shape),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(filter_shape[<span class="number">0</span>],)),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape(self.image_shape)</div><div class="line">        conv_out = conv.conv2d(</div><div class="line">            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,</div><div class="line">            image_shape=self.image_shape)</div><div class="line">        pooled_out = downsample.max_pool_2d(</div><div class="line">            input=conv_out, ds=self.poolsize, ignore_border=<span class="keyword">True</span>)</div><div class="line">        self.output = self.activation_fn(</div><div class="line">            pooled_out + self.b.dimshuffle(<span class="string">'x'</span>, <span class="number">0</span>, <span class="string">'x'</span>, <span class="string">'x'</span>))</div><div class="line">        self.output_dropout = self.output <span class="comment"># no dropout in the convolutional layers</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullyConnectedLayer</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="number">0.0</span>)</span>:</span></div><div class="line">        self.n_in = n_in</div><div class="line">        self.n_out = n_out</div><div class="line">        self.activation_fn = activation_fn</div><div class="line">        self.p_dropout = p_dropout</div><div class="line">        <span class="comment"># Initialize weights and biases</span></div><div class="line">        self.w = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(</div><div class="line">                    loc=<span class="number">0.0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=(n_in, n_out)),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'w'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.asarray(np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=(n_out,)),</div><div class="line">                       dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</div><div class="line">        self.output = self.activation_fn(</div><div class="line">            (<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</div><div class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</div><div class="line">        self.inpt_dropout = dropout_layer(</div><div class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</div><div class="line">        self.output_dropout = self.activation_fn(</div><div class="line">            T.dot(self.inpt_dropout, self.w) + self.b)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"Return the accuracy for the mini-batch."</span></div><div class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxLayer</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_in, n_out, p_dropout=<span class="number">0.0</span>)</span>:</span></div><div class="line">        self.n_in = n_in</div><div class="line">        self.n_out = n_out</div><div class="line">        self.p_dropout = p_dropout</div><div class="line">        <span class="comment"># Initialize weights and biases</span></div><div class="line">        self.w = theano.shared(</div><div class="line">            np.zeros((n_in, n_out), dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'w'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.zeros((n_out,), dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</div><div class="line">        self.output = softmax((<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</div><div class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</div><div class="line">        self.inpt_dropout = dropout_layer(</div><div class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</div><div class="line">        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self, net)</span>:</span></div><div class="line">        <span class="string">"Return the log-likelihood cost."</span></div><div class="line">        <span class="keyword">return</span> -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[<span class="number">0</span>]), net.y])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"Return the accuracy for the mini-batch."</span></div><div class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#### Miscellanea</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="string">"Return the size of the dataset `data`."</span></div><div class="line">    <span class="keyword">return</span> data[<span class="number">0</span>].get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_layer</span><span class="params">(layer, p_dropout)</span>:</span></div><div class="line">    srng = shared_randomstreams.RandomStreams(</div><div class="line">        np.random.RandomState(<span class="number">0</span>).randint(<span class="number">999999</span>))</div><div class="line">    mask = srng.binomial(n=<span class="number">1</span>, p=<span class="number">1</span>-p_dropout, size=layer.shape)</div><div class="line">    <span class="keyword">return</span> layer*T.cast(mask, theano.config.floatX)</div></pre></td></tr></table></figure></p>

    

    
</div>


                

                <!-- Post Comments -->
                
                    







                
            </div>

            <!-- Post Prev & Next Nav -->
            <nav class="material-nav mdl-color-text--grey-50 mdl-cell mdl-cell--12-col">
    <!-- Prev Nav -->
    
        <a href="/2017/03/12/note_nndl_backpropagation/" id="post_nav-newer" class="prev-content">
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_back</i>
            </button>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            新篇
        </a>
    

    <!-- Section Spacer -->
    <div class="section-spacer"></div>

    <!-- Next Nav -->
    
        <a href="/2017/03/12/note_nndl_improve_neural_networks/" id="post_nav-older" class="next-content">
            旧篇
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_forward</i>
            </button>
        </a>
    
</nav>

        </div>
    </div>



                    
                        <!-- Overlay For Active Sidebar -->
<div class="sidebar-overlay"></div>

<!-- Material sidebar -->
<aside id="sidebar" class="sidebar sidebar-colored sidebar-fixed-left" role="navigation">
    <div id="sidebar-main">
        <!-- Sidebar Header -->
        <div class="sidebar-header header-cover" style="background-image: url(/img/sidebar_header.png);">
    <!-- Top bar -->
    <div class="top-bar"></div>

    <!-- Sidebar toggle button -->
    <button type="button" class="sidebar-toggle mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon" style="display: initial;" data-upgraded=",MaterialButton,MaterialRipple">
        <i class="material-icons">clear_all</i>
        <span class="mdl-button__ripple-container">
            <span class="mdl-ripple">
            </span>
        </span>
    </button>

    <!-- Sidebar Avatar -->
    <div class="sidebar-image">
        <img src="/img/avatar.png" alt="John Doe's avatar">
    </div>

    <!-- Sidebar Email -->
    <a data-toggle="dropdown" class="sidebar-brand" href="#settings-dropdown">
        youremail@email.com
        <b class="caret"></b>
    </a>
</div>


        <!-- Sidebar Navigation  -->
        <ul class="nav sidebar-nav">
    <!-- User dropdown  -->
    <li class="dropdown">
        <ul id="settings-dropdown" class="dropdown-menu">
            
                <li>
                    <a href="#" target="_blank" title="Email Me">
                        
                            <i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">email</i>
                        
                        Email Me
                    </a>
                </li>
            
        </ul>
    </li>

    <!-- Homepage -->
    
        <li id="sidebar-first-li">
            <a href="/" target="_self">
                
                    <i class="material-icons sidebar-material-icons">home</i>
                
                主页
            </a>
        </li>
        
    

    <!-- Archives  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">inbox</i>
                
                    归档
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
            <li>
                <a class="sidebar_archives-link" href="/archives/2017/04/">四月 2017<span class="sidebar_archives-count">28</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/03/">三月 2017<span class="sidebar_archives-count">37</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/02/">二月 2017<span class="sidebar_archives-count">12</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/01/">一月 2017<span class="sidebar_archives-count">12</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/12/">十二月 2016<span class="sidebar_archives-count">18</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/11/">十一月 2016<span class="sidebar_archives-count">24</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/10/">十月 2016<span class="sidebar_archives-count">39</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/09/">九月 2016<span class="sidebar_archives-count">32</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/08/">八月 2016<span class="sidebar_archives-count">32</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/07/">七月 2016<span class="sidebar_archives-count">21</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/06/">六月 2016<span class="sidebar_archives-count">33</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/05/">五月 2016<span class="sidebar_archives-count">20</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/04/">四月 2016<span class="sidebar_archives-count">15</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/03/">三月 2016<span class="sidebar_archives-count">19</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/02/">二月 2016<span class="sidebar_archives-count">16</span></a></li><li><a class="sidebar_archives-link" href="/archives/2016/01/">一月 2016<span class="sidebar_archives-count">19</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/12/">十二月 2015<span class="sidebar_archives-count">20</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/11/">十一月 2015<span class="sidebar_archives-count">30</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/10/">十月 2015<span class="sidebar_archives-count">30</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/09/">九月 2015<span class="sidebar_archives-count">27</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/08/">八月 2015<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/07/">七月 2015<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/06/">六月 2015<span class="sidebar_archives-count">10</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/05/">五月 2015<span class="sidebar_archives-count">7</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/04/">四月 2015<span class="sidebar_archives-count">11</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/03/">三月 2015<span class="sidebar_archives-count">10</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/02/">二月 2015<span class="sidebar_archives-count">14</span></a></li><li><a class="sidebar_archives-link" href="/archives/2015/01/">一月 2015<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/12/">十二月 2014<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/11/">十一月 2014<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/10/">十月 2014<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/09/">九月 2014<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/08/">八月 2014<span class="sidebar_archives-count">9</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/07/">七月 2014<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/06/">六月 2014<span class="sidebar_archives-count">7</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/05/">五月 2014<span class="sidebar_archives-count">11</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/04/">四月 2014<span class="sidebar_archives-count">32</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/03/">三月 2014<span class="sidebar_archives-count">34</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/02/">二月 2014<span class="sidebar_archives-count">14</span></a></li><li><a class="sidebar_archives-link" href="/archives/2014/01/">一月 2014<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2013/12/">十二月 2013<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2013/07/">七月 2013<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2013/06/">六月 2013<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2013/04/">四月 2013<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2013/03/">三月 2013<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2013/02/">二月 2013<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2013/01/">一月 2013<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2012/12/">十二月 2012<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2012/10/">十月 2012<span class="sidebar_archives-count">6</span></a></li><li><a class="sidebar_archives-link" href="/archives/2012/09/">九月 2012<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2012/07/">七月 2012<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2012/06/">六月 2012<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2012/05/">五月 2012<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2011/08/">八月 2011<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2011/07/">七月 2011<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2011/06/">六月 2011<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2011/05/">五月 2011<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2011/04/">四月 2011<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2011/03/">三月 2011<span class="sidebar_archives-count">5</span></a></li><li><a class="sidebar_archives-link" href="/archives/2011/02/">二月 2011<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2010/12/">十二月 2010<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2010/11/">十一月 2010<span class="sidebar_archives-count">4</span></a></li><li><a class="sidebar_archives-link" href="/archives/2010/09/">九月 2010<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2010/07/">七月 2010<span class="sidebar_archives-count">4</span></a></li><li><a class="sidebar_archives-link" href="/archives/2010/06/">六月 2010<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2010/02/">二月 2010<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2010/01/">一月 2010<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2009/09/">九月 2009<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2009/07/">七月 2009<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2009/05/">五月 2009<span class="sidebar_archives-count">5</span></a></li><li><a class="sidebar_archives-link" href="/archives/2009/04/">四月 2009<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2009/03/">三月 2009<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2009/02/">二月 2009<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2008/07/">七月 2008<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2008/06/">六月 2008<span class="sidebar_archives-count">1</span></a>
            </ul>
        </li>
        
    

    <!-- Categories  -->
    

    <!-- Pages  -->
    

    <!-- Article Number  -->
    
</ul>


        <!-- Sidebar Footer -->
        <!--
I'm glad you use this theme, the development is no so easy, I hope you can keep the copyright, I will thank you so much.
If you still want to delete the copyrights, could you still retain the first one? Which namely "Theme Material"
It will not impact the appearance and can give developers a lot of support :)

很高兴您使用并喜欢该主题，开发不易 十分谢谢与希望您可以保留一下版权声明。
如果您仍然想删除的话 能否只保留第一项呢？即 "Theme Material"
它不会影响美观并可以给开发者很大的支持和动力。 :)
-->

<!-- Sidebar Divider -->

    <div class="sidebar-divider"></div>


<!-- Theme Material -->

    <a href="https://github.com/viosey/hexo-theme-material"  class="sidebar-footer-text-a" target="_blank">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
            主题 - Material
            <span class="sidebar-badge badge-circle">i</span>
        </div>
    </a>


<!-- Help & Support -->
<!--

-->

<!-- Feedback -->
<!--

-->

<!-- About Theme -->
<!--

-->

    </div>

    <!-- Sidebar Image -->
    

</aside>

                    

                    
                        <!-- Footer Top Button -->
                        <div class="toTop-wrap">
    <a href="#top" class="toTop">
        <i class="material-icons footer_top-i">expand_less</i>
    </a>
</div>

                    

                    <!--Footer-->
<footer class="mdl-mini-footer" id="bottom">
    
        <!-- Paradox Footer Left Section -->
        <div class="mdl-mini-footer--left-section sns-list">
    <!-- Twitter -->
    
        <a href="https://twitter.com/twitter" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn" style="background-image: url(/img/footer/footer_ico-twitter.svg);">
                <span class="visuallyhidden">Twitter</span>
            </button><!--
     --></a>
    

    <!-- Facebook -->
    
        <a href="https://www.facebook.com/facebook" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn" style="background-image: url(/img/footer/footer_ico-facebook.svg);">
                <span class="visuallyhidden">Facebook</span>
            </button><!--
     --></a>
    

    <!-- Google + -->
    
        <a href="https://www.google.com/" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn" style="background-image: url(/img/footer/footer_ico-gplus.svg);">
                <span class="visuallyhidden">Google Plus</span>
            </button><!--
     --></a>
    

    <!-- Weibo -->
    

    <!-- Instagram -->
    

    <!-- Tumblr -->
    

    <!-- Github -->
    

    <!-- LinkedIn -->
    

    <!-- Zhihu -->
    

    <!-- Bilibili -->
    

    <!-- Telegram -->
    
</div>


        <!--Copyright-->
        <div id="copyright">
            Copyright&nbsp;©&nbsp;
            <script type="text/javascript">
                var fd = new Date();
                document.write(fd.getFullYear());
            </script>
            &nbsp;Hexo
        </div>

        <!-- Paradox Footer Right Section -->

        <!--
        I am glad you use this theme, the development is no so easy, I hope you can keep the copyright.
        It will not impact the appearance and can give developers a lot of support :)

        很高兴您使用该主题，开发不易，希望您可以保留一下版权声明。
        它不会影响美观并可以给开发者很大的支持。 :)
        -->

        <div class="mdl-mini-footer--right-section">
            <div>
                <div class="footer-develop-div">Powered by <a href="https://hexo.io" target="_blank" class="footer-develop-a">Hexo</a></div>
                <div class="footer-develop-div">Theme - <a href="https://github.com/viosey/hexo-theme-material" target="_blank" class="footer-develop-a">Material</a></div>
            </div>
        </div>
    
</footer>


                    <!-- Import File -->

    <script src="/js/lazyload.min.js"></script>
    <script src="/js/js.min.js"></script>



    <script src="/js/nprogress.js"></script>


<script type="text/javascript">
    NProgress.configure({
        showSpinner: true
    });
    NProgress.start();
    $('#nprogress .bar').css({
        'background': '#29d'
    });
    $('#nprogress .peg').css({
        'box-shadow': '0 0 10px #29d, 0 0 15px #29d'
    });
    $('#nprogress .spinner-icon').css({
        'border-top-color': '#29d',
        'border-left-color': '#29d'
    });
    setTimeout(function() {
        NProgress.done();
        $('.fade').removeClass('out');
    }, 800);
</script>
















<!-- Window Load-->
<script>
    $(window).load(function() {
        // Post_Toc parent position fixed
        $('.post-toc-wrap').parent('.mdl-menu__container').css('position', 'fixed');
    });
</script>

<!-- MathJax Load-->

<script>
    <!-- Offer LazyLoad -->
    queue.offer(function(){
        $('.lazy').lazyload({
            effect : 'show'
        });
    });

    <!-- Start Queue -->
    $(document).ready(function(){
        setInterval(function(){
            queue.execNext();
        },200);
    });
</script>

                </main>
            </div>
        </body>
    
</html>
